{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b071bc",
   "metadata": {},
   "source": [
    "# Assignment 08 Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b9a79a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faed445a",
   "metadata": {},
   "source": [
    "#### 1. Using our own terms and diagrams, explain INCEPTIONNET ARCHITECTURE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fe38e5",
   "metadata": {},
   "source": [
    "    Inception is a deep convolutional neural network architecture that was introduced in 2014. It won the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC14). It was mostly developed by Google researchers. Inception’s name was given after the eponym movie.\n",
    "\n",
    "    Inception module:\n",
    "        In Convolutional Neural Networks (CNNs), a large part of the work is to choose the right layer to apply, among the most common options (1x1 filter, 3x3 filter, 5x5 filter or max-pooling). All we need is to find the optimal local construction and to repeat it spatially.\n",
    "\n",
    "  <img src='https://maelfabien.github.io/assets/images/inception.jpg' />\n",
    "  \n",
    "  \n",
    "    As these “Inception modules” are stacked on top of each other, their output correlation statistics are bound to vary: as features of higher abstraction are captured by higher layers, their spatial concentration is expected to decrease suggesting that the ratio of 3×3 and 5×5 convolutions should increase as we move to higher layers.\n",
    "\n",
    "    However, the computational cost of such a solution highly increases. For this reason, in the figure b, dimension reduction through 1X1 convolutions are used as dimension reduction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5c623d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0804ecc1",
   "metadata": {},
   "source": [
    "#### 2. Describe the Inception block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd6298b",
   "metadata": {},
   "source": [
    "    Inception module:\n",
    "        In Convolutional Neural Networks (CNNs), a large part of the work is to choose the right layer to apply, among the most common options (1x1 filter, 3x3 filter, 5x5 filter or max-pooling). All we need is to find the optimal local construction and to repeat it spatially.\n",
    "        \n",
    "  <img src='https://maelfabien.github.io/assets/images/inception.jpg' />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806ddb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a300559d",
   "metadata": {},
   "source": [
    "#### 3. What is the DIMENSIONALITY REDUCTION LAYER (1 LAYER CONVOLUTIONAL)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148e4da",
   "metadata": {},
   "source": [
    "    we use the 1x1 convolutional filters to reduce dimensionality in the filter dimension. As I explained above, these 1x1 conv layers can be used in general to change the filter space dimensionality (either increase or decrease) and in the Inception architecture we see how effective these 1x1 filters can be for dimensionality reduction, explicitly in the filter dimension space, not the spatial dimension space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89677710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4246d2fe",
   "metadata": {},
   "source": [
    "#### 4. THE IMPACT OF REDUCING DIMENSIONALITY ON NETWORK PERFORMANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9b8d76",
   "metadata": {},
   "source": [
    "    So, it will reduce the number of parameter, so the training time will be reduced, and the inference of the model will be quicker. And also it will prevent the model from overfitting the trainig data and generalize well on both test and train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b7783f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76b8aa18",
   "metadata": {},
   "source": [
    "#### 5. Mention three components. Style GoogLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7783cf53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bf4effd",
   "metadata": {},
   "source": [
    "#### 6. Using our own terms and diagrams, explain RESNET ARCHITECTURE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e845515",
   "metadata": {},
   "source": [
    "       We assume that the desired underlying mapping we want to obtain by learning is , to be used as input to the activation function on the top. On the left, the portion within the dotted-line box must directly learn the mapping . On the right, the portion within the dotted-line box needs to learn the residual mapping , which is how the residual block derives its name. If the identity mapping  is the desired underlying mapping, the residual mapping amounts to  and it is thus easier to learn: we only need to push the weights and biases of the upper weight layer (e.g., fully connected layer and convolutional layer) within the dotted-line box to zero. The right figure illustrates the residual block of ResNet, where the solid line carrying the layer input  to the addition operator is called a residual connection (or shortcut connection). With residual blocks, inputs can forward propagate faster through the residual connections across layers. In fact, the residual block can be thought of as a special case of the multi-branch Inception block: it has two branches one of which is the identity mapping.\n",
    "       \n",
    "  <img src='https://d2l.ai/_images/residual-block.svg' />\n",
    "      \n",
    "      The first two layers of ResNet are the same as those of the GoogLeNet we described before: the (7, 7)  convolutional layer with 64 output channels and a stride of 2 is followed by the (3, 3) max-pooling layer with a stride of 2. The difference is the batch normalization layer added after each convolutional layer in ResNet.\n",
    "      \n",
    "      GoogLeNet uses four modules made up of Inception blocks. However, ResNet uses four modules made up of residual blocks, each of which uses several residual blocks with the same number of output channels. The number of channels in the first module is the same as the number of input channels. Since a max-pooling layer with a stride of 2 has already been used, it is not necessary to reduce the height and width. In the first residual block for each of the subsequent modules, the number of channels is doubled compared with that of the previous module, and the height and width are halved.\n",
    "      \n",
    "      \n",
    "  <img src='https://d2l.ai/_images/resnet-block.svg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fac40b1",
   "metadata": {},
   "source": [
    "#### 7. What do Skip Connections entail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f19fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36e94190",
   "metadata": {},
   "source": [
    "#### 8. What is the definition of a residual Block?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add8c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84baff66",
   "metadata": {},
   "source": [
    "#### 9. How can transfer learning help with problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6765579e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dff43a28",
   "metadata": {},
   "source": [
    "#### 10. What is transfer learning, and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d13e39d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d9750dd",
   "metadata": {},
   "source": [
    "#### 11.HOW DO NEURAL NETWORKS LEARN FEATURES? 11. HOW DO NEURAL NETWORKS LEARN FEATURES?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00c6819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57d70d79",
   "metadata": {},
   "source": [
    "#### 12. WHY IS FINE-TUNING BETTER THAN START-UP TRAINING?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99331be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
