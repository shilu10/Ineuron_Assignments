{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b071bc",
   "metadata": {},
   "source": [
    "# Assignment 02 Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3140b64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faed445a",
   "metadata": {},
   "source": [
    "#### 1. Explain convolutional neural network, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f4cd1c",
   "metadata": {},
   "source": [
    "    So, Like Artifical Neural Network, where every neurons connects to eachother neurons in other layers for the parameter sharing, we got a Convolutional Neural Network (CNN) for processing the image, which will do the convolutional operation to extract the image features from the image and use pooling layers to handle the problem of shift-invariant. \n",
    "    \n",
    "    When we use convolutions instead of (or in addition to) regular linear layers we create a convolutional neural network (CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9be921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0804ecc1",
   "metadata": {},
   "source": [
    "#### 2. How does refactoring parts of your neural network definition favor you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff7da2f",
   "metadata": {},
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a300559d",
   "metadata": {},
   "source": [
    "#### 3. What does it mean to flatten? Is it necessary to include it in the MNIST CNN? What is the reason for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ced60c",
   "metadata": {},
   "source": [
    "    Flattening is converting the data into a 1-dimensional array for inputting it to the next layer. We flatten the output of the convolutional layers to create a single long feature vector. And it is connected to the final classification model, which is called a fully-connected layer. In other words, we put all the pixel data in one line and make connections with the final layer. And once again. What is the final layer for? The classification of ‘the cats and dogs.’\n",
    "    \n",
    "  <img src='https://miro.medium.com/v2/resize:fit:786/format:webp/0*usI_HmpFeF2iPBEM.png' />\n",
    "  \n",
    "    For every CNN network/model, we need to flatten the feature map, to pass it to the fully connected layers, which does a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c425f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4246d2fe",
   "metadata": {},
   "source": [
    "#### 4. What exactly does NCHW stand for?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c3d9b9",
   "metadata": {},
   "source": [
    "    Let’s describe the order in which the tensor values are laid out in memory for one of the very popular formats, NCHW. The [a:?] marks refer to the jumps shown in the picture below, which shows the 1D representation of an NCHW tensor in memory.\n",
    "        1. N - Number of batch.\n",
    "        2. C - channels.\n",
    "        3. H - Hright.\n",
    "        4. W - Width\n",
    "    \n",
    "    We also have a another data format to store the input image data in the memory, which is NHWC. where the C is at the end of tensor.\n",
    "    \n",
    "    In Tensorflow, we can specify, whether to use NCHW or NHWC during trianing process through the data_format=\"channel_first\" -> uses NCHW and data_format=\"channel_last\" -> NHWC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b74b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76b8aa18",
   "metadata": {},
   "source": [
    "#### 5. Why are there 7*7*(1168-16) multiplications in the MNIST CNN's third layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91e1640",
   "metadata": {},
   "source": [
    "    There are 1168 parameters for that layer, and ignoring the 16 parameters (=number of filters) of the bias, the (1168-16) parameters is applied to the 7x7 grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e64af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bf4effd",
   "metadata": {},
   "source": [
    "#### 6.Explain definition of  receptive field?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19831fb6",
   "metadata": {},
   "source": [
    "    Local Connectivity. When dealing with high-dimensional inputs such as images, as we saw above it is impractical to connect neurons to all neurons in the previous volume. Instead, we will connect each neuron to only a local region of the input volume. The spatial extent of this connectivity is a hyperparameter called the receptive field of the neuron (equivalently this is the filter size). The extent of the connectivity along the depth axis is always equal to the depth of the input volume. It is important to emphasize again this asymmetry in how we treat the spatial dimensions (width and height) and the depth dimension: The connections are local in 2D space (along width and height), but always full along the entire depth of the input volume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fac40b1",
   "metadata": {},
   "source": [
    "#### 7. What is the scale of an activation's receptive field after two stride-2 convolutions? What is the reason for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed44abaf",
   "metadata": {},
   "source": [
    "    The size of the receptive field increases the deeper we are in the network. After two stride 2 convolutions, the receptive field is 7x7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c151f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36e94190",
   "metadata": {},
   "source": [
    "#### 8. What is the tensor representation of a color image?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3760952d",
   "metadata": {},
   "source": [
    "    We have different colorspaces, that represents the image, colorspaces like,\n",
    "        1. RGB.\n",
    "        2. BGR.\n",
    "        3. CMY.\n",
    "        4. HSI.\n",
    "    \n",
    "    So, these are different colorspaces, that can represent the image, so we can store the image data in the tensor, using rank-3 tensor.\n",
    "    \n",
    "    (H, W, C) -> (height, Width, Channels). Channels is based on type of colorspaces that image is based on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e07ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84baff66",
   "metadata": {},
   "source": [
    "#### 9. How does a color input interact with a convolution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a212bf",
   "metadata": {},
   "source": [
    "    So, for 3d images or different channeled feature maps. we will use the kernels will be same dimension of the feature maps or images.\n",
    "    \n",
    "    eg: if image is 3d ,then the kernel size is,\n",
    "   <img src = 'https://res.cloudinary.com/practicaldev/image/fetch/s--KbmVQKiU--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://i.ibb.co/3zC5NnJ/3d-filter.png' />\n",
    "   \n",
    "    The depth of the filter will be chosen to match the number of color channels and our color image.\n",
    "    \n",
    "   <img src='https://res.cloudinary.com/practicaldev/image/fetch/s--8md8tOUr--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://i.ibb.co/qjH9BZT/filter-with-depth.png' />\n",
    "   \n",
    "    This is because we're going to convolve each color channel with its own two-dimensional filter. Therefore, if we're working with RGB images, our 3D filter will have a depth of three.\n",
    "    \n",
    "   <img src='https://res.cloudinary.com/practicaldev/image/fetch/s--Hr5IPtCf--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://i.ibb.co/FJNtTNm/3d-depth.png' />\n",
    "   \n",
    "    The convolutions will be carried out in exactly the same way as for grayscale images. The only difference is that now we have to perform three convolutions instead of 1.\n",
    "    \n",
    "  <img src='https://res.cloudinary.com/practicaldev/image/fetch/s--UVvkrSuT--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://i.ibb.co/j5wb9Np/convolutions-start.png' />\n",
    "  \n",
    "    When we specify number of filters in tensorflow conv2d, which is not same as the depth it is just specifying the number of different filters we need to learn. the depth will automatically detected using the image or feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f57998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
