{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b071bc",
   "metadata": {},
   "source": [
    "# Assignment 03 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed445a",
   "metadata": {},
   "source": [
    "#### 1. After each stride-2 conv, why do we double the number of filters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c4c465",
   "metadata": {},
   "source": [
    "    When we use a stride-2 convolution, we often increase the number of features because we’re decreasing the number of activations in the activation map by a factor of 4; we don’t want to decrease the capacity of a layer by too much at a time.”\n",
    "\n",
    "    There is one bias for each channel. (Sometimes channels are called features or filters when they are not input channels.) The output shape is 64x4x14x14, and this will therefore become the input shape to the next layer. The next layer, according to summary, has 296 parameters. Let’s ignore the batch axis to keep things simple. So for each of 14*14=196 locations we are multiplying 296-8=288 weights (ignoring the bias for simplicity), so that’s 196*288=56_448 multiplications at this layer. The next layer will have 7*7*(1168-16)=56_448 multiplications.\n",
    "\n",
    "    What happened here is that our stride-2 convolution halved the grid size from 14x14 to 7x7, and we doubled the number of filters from 8 to 16, resulting in no overall change in the amount of computation. If we left the number of channels the same in each stride-2 layer, the amount of computation being done in the net would get less and less as it gets deeper. But we know that the deeper layers have to compute semantically rich features (such as eyes or fur), so we wouldn’t expect that doing less computation would make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ec407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0804ecc1",
   "metadata": {},
   "source": [
    "#### 2. Why do we use a larger kernel with MNIST (with simple cnn) in the first conv?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f8e675",
   "metadata": {},
   "source": [
    "    With the first layer, if the kernel size is 3x3, with four output filters, then nine pixels are being used to produce 8 output numbers so there is not much learning since input and output size are almost the same. Neural networks will only create useful features if they’re forced to do so—that is, if the number of outputs from an operation is significantly smaller than the number of inputs. To fix this, we can use a larger kernel in the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c27783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a300559d",
   "metadata": {},
   "source": [
    "#### 3. What data is saved by ActivationStats for each layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833afd84",
   "metadata": {},
   "source": [
    "    It records the mean, standard deviation, and histogram of activations of every trainable layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649bf04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4246d2fe",
   "metadata": {},
   "source": [
    "#### 4. How do we get a learner's callback after they've completed training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58671753",
   "metadata": {},
   "source": [
    "    They are available with the Learner object with the same name as the callback class, but in snake_case. For example, the Recorder callback is available through learn.recorder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ea061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76b8aa18",
   "metadata": {},
   "source": [
    "#### 5. What are the drawbacks of activations above zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58bb148",
   "metadata": {},
   "source": [
    "    Activations near zero are problematic because it means we have computation in the model that’s doing nothing at all (since multiplying by zero gives zero). When you have some zeros in one layer, they will therefore generally carry over to the next layer… which will then create more zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b7488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bf4effd",
   "metadata": {},
   "source": [
    "#### 6.Draw up the benefits and drawbacks of practicing in larger batches?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0332b6",
   "metadata": {},
   "source": [
    "    The gradients are more accurate since they’re calculated from more data, but a larger batch size means fewer batches per epoch, which means less opportunities for the model to update weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b542c7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fac40b1",
   "metadata": {},
   "source": [
    "#### 7. Why should we avoid starting training with a high learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e08d43a",
   "metadata": {},
   "source": [
    "    Our initial weights are not well suited to the task we’re trying to solve. Therefore, it is dangerous to begin training with a high learning rate: we may very well make the training diverge instantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d40727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36e94190",
   "metadata": {},
   "source": [
    "#### 8. What are the pros of studying with a high rate of learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf3ab0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84baff66",
   "metadata": {},
   "source": [
    "#### 9. Why do we want to end the training with a low learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a7d2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
