{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc44854",
   "metadata": {},
   "source": [
    "# Assignment 06 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35eaef1",
   "metadata": {},
   "source": [
    "#### 1.\tWhat are Vanilla autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d93c8d",
   "metadata": {},
   "source": [
    "    Autoencoders are simple neural architectures. They are basically a form of compression, similar to the way an audio file is compressed using MP3, or an image file is compressed using JPEG.\n",
    "    \n",
    "<img src='https://miro.medium.com/v2/resize:fit:786/format:webp/1*oUbsOnYKX5DEpMOK3pH_lg.png' />\n",
    "\n",
    "    Autoencoders are closely related to principal component analysis (PCA). In fact, if the activation function used within the autoencoder is linear within each layer, the latent variables present at the bottleneck (the smallest layer in the network, aka. code) directly correspond to the principal components from PCA. Generally, the activation function used in autoencoders is non-linear, typical activation functions are ReLU (Rectified Linear Unit) and sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1418bac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c478edb2",
   "metadata": {},
   "source": [
    "#### 2.\tWhat are Sparse autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d170a7a3",
   "metadata": {},
   "source": [
    "    A sparse autoencoder, counterintuitively, has a larger latent dimension than the input or output dimensions. However, each time the network is run, only a small fraction of the neurons fires, meaning that the network is inherently ‘sparse’. This is similar to a denoising autoencoder in the sense that it is also a form of regularization to reduce the propensity for the network to overfit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb759d62",
   "metadata": {},
   "source": [
    "#### 3.\tWhat are Denoising autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65deba1e",
   "metadata": {},
   "source": [
    "    There are several other types of autoencoders. One of the most commonly used is a denoising autoencoder, which will analyze with Keras later in this tutorial. These autoencoders add some white noise to the data prior to training but compare the error to the original image when training. This forces the network to not become overfit to arbitrary noise present in images. We will use this later to remove creases and darkened areas from scanned images of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639b7b3",
   "metadata": {},
   "source": [
    "#### 4.\tWhat are Convolutional autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea0741",
   "metadata": {},
   "source": [
    "    As stated in the previous section, autoencoders are deep learning architectures capable of reconstructing data instances from their feature vectors. They work on all sorts of data but this article is primarily concerned with their application on image data. An autoencoder is made up of 3 main components; namely, an encoder, a bottleneck and a decoder.\n",
    "    \n",
    "  <img src='https://blog.paperspace.com/content/images/2022/08/autoencoder_structure.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78175e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c4d0589",
   "metadata": {},
   "source": [
    "#### 5.\tWhat are Stacked autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40873530",
   "metadata": {},
   "source": [
    "    A stacked autoencoder is a neural network consist several layers of sparse autoencoders where output of each hidden layer is connected to the input of the successive hidden layer.\n",
    "    Stacked autoencoder are used for P300 Component Detection and Classification of 3D Spine Models in Adolescent Idiopathic Scoliosis in medical science. Classification of the rich and complex variability of spinal deformities is critical for comparisons between treatments and for long-term patient follow-ups.\n",
    "    \n",
    " <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*7H9VQlN94-wv7Ianqt6GZg.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e593ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "120eaa88",
   "metadata": {},
   "source": [
    "#### 6.\tExplain Extractive summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54891cf",
   "metadata": {},
   "source": [
    "    Extractive summarization methods work just like that. It takes the text, ranks all the sentences according to the understanding and relevance of the text, and presents you with the most important sentences.\n",
    "    \n",
    "    This method does not create new words or phrases, it just takes the already existing words and phrases and presents only that. You can imagine this as taking a page of text and marking the most important sentences using a highlighter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fbc329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c248e57d",
   "metadata": {},
   "source": [
    "#### 7.\tExplain Abstractive summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038393d1",
   "metadata": {},
   "source": [
    "    Abstractive summarization, on the other hand, tries to guess the meaning of the whole text and presents the meaning to you.\n",
    "    \n",
    "    It creates words and phrases, puts them together in a meaningful way, and along with that, adds the most important facts found in the text. This way, abstractive summarization techniques are more complex than extractive summarization techniques and are also computationally more expensive.\n",
    "    \n",
    "    And also abstractive summarization, is a human-level summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3056726a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f245f2ef",
   "metadata": {},
   "source": [
    "#### 8.\tExplain Beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cb84d8",
   "metadata": {},
   "source": [
    "    It is used in the sequence-to-sequence models, where the model outputs a text, applications like machine translation, chatbots, text summarization, language modelling.\n",
    "    \n",
    "    Beam search is used in the last layer of these applications to produce their output text(document), another commonly known algorithm is `greedy search`.\n",
    "    \n",
    "    In sequence-to-sequence models, the decoders, will work in time step fashion, and it has a fully-connected layer and softmax at the end.\n",
    "    \n",
    "   <img src='https://miro.medium.com/v2/resize:fit:786/format:webp/1*GkG_5wg57IpkU8F84nJubQ.png' />\n",
    "   \n",
    "     - search algorithm, will say, how we are gonna interpret those results at each time steps.\n",
    "     \n",
    "     - In greedy search, when the model outputs a probability distribution over the vocbulary(words), the greedy search will select the highest probability word(token).\n",
    "     \n",
    "     - In beam search, there are two improvement are made, so it is better than the greedy search, but computationally expensive,\n",
    "         1. With Greedy Search, we took just the single best word at each time step. In contrast, Beam Search expands this and takes the best ’N’ words.\n",
    "         2. With Greedy Search, we considered each position in isolation. Once we had identified the best word for that position, we did not examine what came before it (ie. in the previous position), or after it. In contrast, Beam Search picks the ’N’ best sequences so far and considers the probabilities of the combination of all of the preceding words along with the word in the current position.\n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b70c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0cdc2a3",
   "metadata": {},
   "source": [
    "#### 9. Explain Length normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c95756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e67671d",
   "metadata": {},
   "source": [
    "#### 10. Explain Coverage normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cfeaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8b60772",
   "metadata": {},
   "source": [
    "#### 11. Explain ROUGE metric evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb2233e",
   "metadata": {},
   "source": [
    "    ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It is essentially a set of metrics for evaluating automatic summarization of texts as well as machine translations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
