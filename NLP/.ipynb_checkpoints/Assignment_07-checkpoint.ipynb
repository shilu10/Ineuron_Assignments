{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc44854",
   "metadata": {},
   "source": [
    "# Assignment 07 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35eaef1",
   "metadata": {},
   "source": [
    "#### 1.\tExplain the architecture of BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be20c44a",
   "metadata": {},
   "source": [
    "<img src='https://jalammar.github.io/images/bert-transfer-learning.png' />\n",
    "\n",
    "   \n",
    "   \n",
    "       Bert is trained on large corpus of unlabelled dataset, where it uses the self-supervised learnin method with two pretext task of Masked Modelling and Next Sentence Prediction.\n",
    "       \n",
    "       We use the Bert to extract the feature representation of the text, using those feature representation, we can power the downstream tasks, like text classification, or sentiment analysis using MLP and softmax layers.\n",
    "       \n",
    "       BERT is basically a trained Transformer Encoder stack.\n",
    "       \n",
    "       Two Common BERT model types, are BERT small and BERT large.\n",
    "       \n",
    "   <img src='https://jalammar.github.io/images/bert-base-bert-large-encoders.png' />\n",
    "   \n",
    "   \n",
    "       The first input token is supplied with a special [CLS] token for reasons that will become apparent later on. CLS here stands for Classification.\n",
    "       \n",
    "   <img src='https://jalammar.github.io/images/bert-input-output.png' />\n",
    "   \n",
    "       the bert takes sequence as an input, similar to the transformer, where the inputs will be parallely processed, and it outputs a fixed representation size of 768 dimension for each term in the document. And we are only interested in the CLS token, bcoz it is been shown that the ClS token's vector representation learns better way of representing the text.\n",
    "       \n",
    "   <img src=\"https://jalammar.github.io/images/bert-output-vector.png\" />\n",
    "   \n",
    "       That vector can now be used as the input for a classifier of our choosing. The paper achieves great results by just using a single-layer neural network as the classifier.\n",
    "       \n",
    "   <img src='https://jalammar.github.io/images/bert-classifier.png' />\n",
    "   \n",
    "   \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66a630b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c478edb2",
   "metadata": {},
   "source": [
    "#### 2.\tExplain Masked Language Modeling (MLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b84571",
   "metadata": {},
   "source": [
    "    - Language Models are models that assigns a probability distribution over the sequence of words, and next the word given other context(surronding words).\n",
    "    - We will use self-supervised learning for training the large language modelling, So there are two types of tasks, Pre-text task and Downstream task.\n",
    "    \n",
    "    - Pre-text tasks are tasks that are used in pre-training the language models, without a labels(supervision). So, in NLP, there are multiple pre-text tasks like.\n",
    "        1. Masked Language Modelling.\n",
    "        2. Casual Language Modelling.\n",
    "        3. Next Sentence Prediction. \n",
    "        4. Random Token prediction.\n",
    "        5. Swapped Token Prediction.\n",
    "        6. Translation Language Modelling (TLM)\n",
    "        7. Alternate Language Modelling (ALM)\n",
    "        8. Sentence Boundary Objective (SBO)\n",
    "        9. etc.\n",
    "        \n",
    "    - So, MLM is one type of pre-text task in self-supervised learning in nlp, MLM, is a bidirectional context consideration. Where it will consider both the leftward and rightward context.\n",
    "    - MLM, is used in BERT pretraining.\n",
    "    - CLM, is used in GPT-1 pretraining.\n",
    "    - RTP, is used in ELECTRA pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe555532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb759d62",
   "metadata": {},
   "source": [
    "#### 3.\tExplain Next Sentence Prediction (NSP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faadbd93",
   "metadata": {},
   "source": [
    "    - Language Models are models that assigns a probability distribution over the sequence of words, and next the word given other context(surronding words).\n",
    "    - We will use self-supervised learning for training the large language modelling, So there are two types of tasks, Pre-text task and Downstream task.\n",
    "    \n",
    "    - Pre-text tasks are tasks that are used in pre-training the language models, without a labels(supervision). So, in NLP, there are multiple pre-text tasks like.\n",
    "        1. Masked Language Modelling.\n",
    "        2. Casual Language Modelling.\n",
    "        3. Next Sentence Prediction. \n",
    "        4. Random Token prediction.\n",
    "        5. Swapped Token Prediction.\n",
    "        6. Translation Language Modelling (TLM)\n",
    "        7. Alternate Language Modelling (ALM)\n",
    "        8. Sentence Boundary Objective (SBO)\n",
    "        9. etc.\n",
    "        \n",
    "    - So, NSP is one type of pre-text task in self-supervised learning in nlp, NSP, is a unidirectional context consideration. Where it will consider only the leftward context.\n",
    "    - MLM, NSP are used in BERT pretraining.\n",
    "    - CLM, is used in GPT-1 pretraining.\n",
    "    - RTP, is used in ELECTRA pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cca6625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a639b7b3",
   "metadata": {},
   "source": [
    "#### 4.\tWhat is Matthews evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe0c26a",
   "metadata": {},
   "source": [
    "    MCC is a best single-value classification metric which helps to summarize the confusion matrix or an error matrix. A confusion matrix has four entities:\n",
    "        True positives (TP)\n",
    "        True negatives (TN)\n",
    "        False positives (FP)\n",
    "        False negatives (FN)\n",
    "    And is calculated by the formula:\n",
    "   <img src='https://www.voxco.com/wp-content/uploads/2021/12/Matthews-correlation-coefficient2.png' />\n",
    "   \n",
    "     if the prediction returns good rates for all four of these entities, it is said to be a reliable measure producing high scores. And to suit most correlation coefficients, MCC also ranges between +1 and -1 as:\n",
    "\n",
    "    +1 is the best agreement between the predicted and actual values.\n",
    "    0 is no agreement. Meaning, prediction is random according to the actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b505289a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c4d0589",
   "metadata": {},
   "source": [
    "#### 5.\tWhat is Matthews Correlation Coefficient (MCC)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bcbf17",
   "metadata": {},
   "source": [
    "    MCC is a best single-value classification metric which helps to summarize the confusion matrix or an error matrix. A confusion matrix has four entities:\n",
    "        True positives (TP)\n",
    "        True negatives (TN)\n",
    "        False positives (FP)\n",
    "        False negatives (FN)\n",
    "    And is calculated by the formula:\n",
    "   <img src='https://www.voxco.com/wp-content/uploads/2021/12/Matthews-correlation-coefficient2.png' />\n",
    "   \n",
    "     if the prediction returns good rates for all four of these entities, it is said to be a reliable measure producing high scores. And to suit most correlation coefficients, MCC also ranges between +1 and -1 as:\n",
    "\n",
    "    +1 is the best agreement between the predicted and actual values.\n",
    "    0 is no agreement. Meaning, prediction is random according to the actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73998552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34fa3100",
   "metadata": {},
   "source": [
    "#### 6.\tExplain Semantic Role Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb81347",
   "metadata": {},
   "source": [
    "    n natural language processing, semantic role labeling (also called shallow semantic parsing or slot-filling) is the process that assigns labels to words or phrases in a sentence that indicates their semantic role in the sentence, such as that of an agent, goal, or result.\n",
    "\n",
    "    It serves to find the meaning of the sentence. To do this, it detects the arguments associated with the predicate or verb of a sentence and how they are classified into their specific roles. A common example is the sentence \"Mary sold the book to John.\" The agent is \"Mary,\" the predicate is \"sold\" (or rather, \"to sell,\") the theme is \"the book,\" and the recipient is \"John.\" Another example is how \"the book belongs to me\" would need two labels such as \"possessed\" and \"possessor\" and \"the book was sold to John\" would need two other labels such as theme and recipient, despite these two clauses being similar to \"subject\" and \"object\" functions.[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72553289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb3a2c3d",
   "metadata": {},
   "source": [
    "#### 7.\tWhy Fine-tuning a BERT model takes less time than pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d340e82f",
   "metadata": {},
   "source": [
    "   <img src='https://miro.medium.com/v2/resize:fit:720/format:webp/1*bYO5tEcRzdHtjHV_P6-4ig.png' />\n",
    "   \n",
    "       Left-hand side the bert is pretraining using two pre-text task (NSP and MLM) in self-supervised fashion. In Right-hand side the pretrained bert is fine-tuned for specific Downstream tasks like NER(Named Entity Recoginition), Question and Answering, etc.\n",
    "       In pretraining the model is trained on larget unlabeled dataset using self-supervised learning, in the finetuning model is trained on small or relatively smaller dataset compared to the larget unlabeled dataset, with only few epochs and adding few mlp layers.\n",
    "       \n",
    "       So, bcoz the model is only trained for few epochs and with relatively smaller dataset than the pre-training and also the learning rate is set to minimum, the fine-tuning of any pre-trained model will be faster than pre-training the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9113dee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "594d1a35",
   "metadata": {},
   "source": [
    "#### 8.\tRecognizing Textual Entailment (RTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bebd217",
   "metadata": {},
   "source": [
    "    Textual Entailment Recognition has been proposed recently as a generic task that captures major semantic inference needs across many NLP applications, such as Question Answering, Information Retrieval, Information Extraction, and Tex\n",
    "  <img src='https://www.researchgate.net/publication/49612115/figure/fig1/AS:669986418851872@1536748618583/Textual-Entailment-System.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a1124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "945029ae",
   "metadata": {},
   "source": [
    "#### 9.\tExplain the decoder stack of  GPT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96441913",
   "metadata": {},
   "source": [
    "    The GPT-2 is built using transformer decoder blocks. BERT, on the other hand, uses transformer encoder blocks. We will examine the difference in a following section. But one key difference between the two is that GPT2, like traditional language models, outputs one token at a time. Let’s for example prompt a well-trained GPT-2 to recite the first law of robotics:\n",
    "    \n",
    "  <img src='https://jalammar.github.io/images/xlnet/gpt-2-output.gif' />\n",
    "  \n",
    "    The way these models actually work is that after each token is produced, that token is added to the sequence of inputs. And that new sequence becomes the input to the model in its next step. This is an idea called “auto-regression”. This is one of the ideas that made RNNs unreasonably effective.\n",
    "    \n",
    "  <img src='https://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif' />\n",
    "  \n",
    "    GPT, is a decoder-only transformer.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8893aca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
