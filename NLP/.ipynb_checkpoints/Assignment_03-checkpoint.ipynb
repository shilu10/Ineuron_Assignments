{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc44854",
   "metadata": {},
   "source": [
    "# Assignment 03 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35eaef1",
   "metadata": {},
   "source": [
    "#### 1.\tExplain the basic architecture of RNN cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5588fd6a",
   "metadata": {},
   "source": [
    "**- Unlike a feed-forward neural network, rnn does'nt process a data in parallel, rnn process a data in sequential way, where output of one-time step is input of another time step in a single sample, this is because of the nature of input data to the rnn, where the input data is a sequential data or time-series data, where there will be a relationship between the each time step, temporal dependencies. That is why it is known as a recurrent, where it will pass the output of one time step to itself. and it also have a `hidden state, also known as Memory state`, where, it will hold a information about the previous time steps, which will be useful in processing the `current time step's hidden state and output`.**\n",
    "\n",
    "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20230518134831/What-is-Recurrent-Neural-Network.webp' />\n",
    "\n",
    "**- The Parameter across the time step in a layer, will be shared. if the parameter is not shared, then the neural network will be just a feed-forward neural network. All the Hidden state weight, input state weight and output state weight are shared across the layer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0cbf46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c478edb2",
   "metadata": {},
   "source": [
    "#### 2.\tExplain Backpropagation through time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20a6c1",
   "metadata": {},
   "source": [
    "**- Normal backpropagation, is a supervised learning kind of algorithm, where it will try to minimize the error of the network, by updating the network parameter, with the help of the network output. Network, takes the input and does a process and outputs an prediction, now the prediction will be compared with the true value, and the loss will be calculated, the the derivative of the loss function, with the respective to the loss is calculated, with the slope the backpropagation algorithm will update the network parameter.**\n",
    "\n",
    "**- But, in RNN, at each time step, it outputs an prediction, so the normal backpropagation won't work here. So we will calculate the backpropagation with repc to time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5121af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb759d62",
   "metadata": {},
   "source": [
    "#### 3.\tExplain Vanishing and exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7748af",
   "metadata": {},
   "source": [
    "**- Through this process, RNNs tend to run into two problems, known as exploding gradients and vanishing gradients. These issues are defined by the size of the gradient, which is the slope of the loss function along the error curve. When the gradient is too small, it continues to become smaller, updating the weight parameters until they become insignificant—i.e. 0. When that occurs, the algorithm is no longer learning. Exploding gradients occur when the gradient is too large, creating an unstable model. In this case, the model weights will grow too large, and they will eventually be represented as NaN. One solution to these issues is to reduce the number of hidden layers within the neural network, eliminating some of the complexity in the RNN model.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a858f220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a639b7b3",
   "metadata": {},
   "source": [
    "#### 4.\tExplain Long short-term memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9250566",
   "metadata": {},
   "source": [
    "**- this is a popular RNN architecture, which was introduced by Sepp Hochreiter and Juergen Schmidhuber as a solution to vanishing gradient problem. In their paper (PDF, 388 KB) (link resides outside IBM), they work to address the problem of long-term dependencies. That is, if the previous state that is influencing the current prediction is not in the recent past, the RNN model may not be able to accurately predict the current state. As an example, let’s say we wanted to predict the italicized words in following, “Alice is allergic to nuts. She can’t eat peanut butter.” The context of a nut allergy can help us anticipate that the food that cannot be eaten contains nuts. However, if that context was a few sentences prior, then it would make it difficult, or even impossible, for the RNN to connect the information. To remedy this, LSTMs have “cells” in the hidden layers of the neural network, which have three gates–an input gate, an output gate, and a forget gate. These gates control the flow of information which is needed to predict the output in the network.  For example, if gender pronouns, such as “she”, was repeated multiple times in prior sentences, you may exclude that from the cell state.**\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:674/1*jikKbzFXCq-IYnFZankIMg.png' />\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:720/0*-Gsbo9tO04t9OGML.gif' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f67350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c4d0589",
   "metadata": {},
   "source": [
    "#### 5.\tExplain Gated recurrent unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b09d323",
   "metadata": {},
   "source": [
    "**- This RNN variant is similar the LSTMs as it also works to address the short-term memory problem of RNN models. Instead of using a “cell state” regulate information, it uses hidden states, and instead of three gates, it has two—a reset gate and an update gate. Similar to the gates within LSTMs, the reset and update gates control how much and which information to retain.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242ceced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34fa3100",
   "metadata": {},
   "source": [
    "#### 6.\tExplain Peephole LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b8c451",
   "metadata": {},
   "source": [
    "**- One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding “peephole connections.” This means that we let the gate layers look at the cell state. In this peephole connection we can see that all the gates are having an input along with the cell state.**\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:720/format:webp/0*9ofTZTHKBGuMwTxd.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300a5325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10593383",
   "metadata": {},
   "source": [
    "#### 7.\tBidirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c68eca",
   "metadata": {},
   "source": [
    "**- Unidirectional RNN, works well in Language Modelling, which is nothing but predicting the next token in a given sequence, where the RNN, needs to predict the final token in the sequence, given previous part of the sequence. In this scenario, we wish only to condition upon the leftward context, and thus the unidirectional chaining of a standard RNN seems appropriate. Because, we getting the information from the leftward, which is enough, there are many other sequence learning tasks contexts where it is perfectly fine to condition the prediction at every time step on both the leftward and the rightward context.**\n",
    "\n",
    "**- one common sequential task often useful as a pretraining exercise prior to fine-tuning a model on an actual task of interest—is to mask out random tokens in a text document and then to train a sequence model to predict the values of the missing tokens. Note that depending on what comes after the blank, the likely value of the missing token changes dramatically:**\n",
    "    \n",
    "   **eg:**\n",
    "\n",
    "        I am ___.\n",
    "\n",
    "        I am ___ hungry.\n",
    "\n",
    "        I am ___ hungry, and I can eat half a pig.\n",
    "\n",
    "        In the first sentence “happy” seems to be a likely candidate. The words “not” and “very” seem plausible in the second sentence, but “not” seems incompatible with the third sentences.\n",
    "        \n",
    "#### Bidirectional RNN\n",
    "**a simple technique transforms any unidirectional RNN into a bidirectional RNN (Schuster and Paliwal, 1997). We simply implement two unidirectional RNN layers chained together in opposite directions and acting on the same input (Fig. 10.4.1). For the first RNN layer, the first input is  and the last input is , but for the second RNN layer, the first input is  and the last input is . To produce the output of this bidirectional RNN layer, we simply concatenate together the corresponding outputs of the two underlying unidirectional RNN layers.**\n",
    "        \n",
    "<img src='https://d2l.ai/_images/birnn.svg' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f780745f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8d13fb1",
   "metadata": {},
   "source": [
    "#### 8.\tExplain the gates of LSTM with equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ae26d6",
   "metadata": {},
   "source": [
    "**- Three types of gates are used in the LSTM, to address the problem of short-term memory. they are.**\n",
    "   \n",
    "   `1. Input Gate.`\n",
    "   \n",
    "   `2. Output Gate.`\n",
    "    \n",
    "   `3. Forget Gate.`\n",
    "\n",
    "**- Input Gate -> determines how much of the input node’s value should be added to the current memory cell internal state.**\n",
    "\n",
    "**-Forget Gate -> determines whether to keep the current value of the memory or flush it.**\n",
    "\n",
    "**-Output Gate -> determines whether the memory cell should influence the output at the current time step.**\n",
    "\n",
    "<img src='https://d2l.ai/_images/lstm-0.svg' />\n",
    "\n",
    "\n",
    "\n",
    "\\begin{split}\\begin{aligned}\n",
    "\\mathbf{I}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i),\\\\\n",
    "\\mathbf{F}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f),\\\\\n",
    "\\mathbf{O}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o),\n",
    "\\end{aligned}\\end{split}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c6694b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7956108f",
   "metadata": {},
   "source": [
    "#### 9.\tExplain BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052666d3",
   "metadata": {},
   "source": [
    "**- Bidirectional LSTM (BiLSTM) is a recurrent neural network used primarily on natural language processing. Unlike standard LSTM, the input flows in both directions, and it’s capable of utilizing information from both sides. It’s also a powerful tool for modeling the sequential dependencies between words and phrases in both directions of the sequence.**\n",
    "\n",
    "**- In summary, BiLSTM adds one more LSTM layer, which reverses the direction of information flow. Briefly, it means that the input sequence flows backward in the additional LSTM layer. Then we combine the outputs from both LSTM layers in several ways, such as average, sum, multiplication, or concatenation.**\n",
    "\n",
    "<img src='https://www.baeldung.com/wp-content/uploads/sites/4/2022/01/bilstm-1-1024x384.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d43e906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "607b9728",
   "metadata": {},
   "source": [
    "#### 10.\tExplain BiGRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7097a346",
   "metadata": {},
   "source": [
    "**- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
