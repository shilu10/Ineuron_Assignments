{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc44854",
   "metadata": {},
   "source": [
    "# Assignment 02 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35eaef1",
   "metadata": {},
   "source": [
    "#### 1.\tWhat are Corpora?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d78cdc",
   "metadata": {},
   "source": [
    "**- Text corpus, is a large collection of structural text data. That can be used for the statistical analysis and training a Neural Networks. It is a collection of multiple documents. eg: collection of wikipedia articles is a corpus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b746d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c478edb2",
   "metadata": {},
   "source": [
    "#### 2.\tWhat are Tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613daeb5",
   "metadata": {},
   "source": [
    "**-Token are generated by the process of Tokenization, where the large sentence and paragraphs are break into smaller word, which provides a better meaning for encoding.**\n",
    "   \n",
    "   eg:\n",
    "        \n",
    "        `How was your day?`\n",
    "        \n",
    "        `\"how\", 'was\" \"your\" \"day\" -> tokens`\n",
    " \n",
    " **- There are different types of tokenization like, `word tokenization, character tokenization, and subword tokenization`, these methods, will generate a tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95427881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb759d62",
   "metadata": {},
   "source": [
    "#### 3.\tWhat are Unigrams, Bigrams, Trigrams?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aac22c",
   "metadata": {},
   "source": [
    "**- Models that assign probabilities to sequences of words are called `language models(LMs)`.**\n",
    "\n",
    "**- The simplest language model that assigns `probabilities to sentences and sequences of words` is the `n-gram`**\n",
    "\n",
    "**-An n-gram is a sequence of N words:**\n",
    "    \n",
    "    – A 1-gram (unigram) is a single word sequence of words like “please” or “ turn”.\n",
    "    \n",
    "    – A 2-gram (bigram) is a two-word sequence of words like “please turn”, “turn your”, or\n",
    "    ”your homework”.\n",
    "    \n",
    "    – A 3-gram (trigram) is a three-word sequence of words like “please turn your”, or “turn your\n",
    "       homework”\n",
    "       \n",
    "**- We can use n-gram models to estimate the probability of the last word of an n-gram given the previous words, and also to assign probabilities to entire word sequences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9130391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a639b7b3",
   "metadata": {},
   "source": [
    "#### 4.\tHow to generate n-grams from text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d101f63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a')\n",
      "('is', 'a', 'very')\n",
      "('a', 'very', 'good')\n",
      "('very', 'good', 'book')\n",
      "('good', 'book', 'to')\n",
      "('book', 'to', 'study')\n"
     ]
    }
   ],
   "source": [
    "# NLTK function to generate ngrams\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    " \n",
    "samplText='this is a very good book to study'\n",
    "NGRAMS=ngrams(sequence=nltk.word_tokenize(samplText), n=3)\n",
    "for grams in NGRAMS:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6fc6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c4d0589",
   "metadata": {},
   "source": [
    "#### 5.\tExplain Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a0083",
   "metadata": {},
   "source": [
    "**- Lemmatization, is a text preprocessing technique, which does a text normalization.**\n",
    "\n",
    "**- The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:**\n",
    "\n",
    "`am, are, is -> be`\n",
    "\n",
    "`car, cars, car's, cars' -> car`\n",
    " \n",
    " **- lemmatization, brings context to the words. So it links words with similar meanings to one word.Lemmatizer algorithms usually also use positional arguments as inputs, such as whether the word is an adjective, noun, or verb.**\n",
    " \n",
    "<img src='https://databasecamp.de/wp-content/uploads/image-768x373.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5c3373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34fa3100",
   "metadata": {},
   "source": [
    "#### 6.\tExplain Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdbf1b0",
   "metadata": {},
   "source": [
    "**- Stemming is the process of removing suffixes from words to create a so-called root word, or a head word.**\n",
    "\n",
    "**- Stemming allows us to standardize words to their base stem regardless of their inflections, which is useful in many applications such as clustering or text classification.**\n",
    "\n",
    "**- Types of Stemming algorithms,\n",
    "    1. Port Stemmer. \n",
    "    2. SnowBall Stemmer.\n",
    "    3. Lanchaster Stemmer.**\n",
    "    \n",
    "**- Sometime, stemming can lead to the problem of `understeaming and oversteaming`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fed787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05471446",
   "metadata": {},
   "source": [
    "#### 7.\tExplain Part-of-speech (POS) tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f554d3",
   "metadata": {},
   "source": [
    "**- Syntax Analysis deals with the arrangement of words to form a structure that makes grammatical sense.**\n",
    "\n",
    "**- Part of Speech is the classification of words based on their role in the sentence. The major POS tags are Nouns, Verbs, Adjectives, Adverbs. This category provides more details about the word and its meaning in the context. A sentence consists of words with a sensible Part of Speech structure.**\n",
    "\n",
    "**- POS tagging refers to the automatic assignment of a tag to words in a given sentence. It converts a sentence into a list of words with their tags. (word, tag).**\n",
    "\n",
    "**- For example, the previous sentence, “Book the flight”, will become a list of each word with its corresponding POS tag – [(“Book”, “Verb”), (“the”, “Det”), (“flight”, “Noun”)].**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625fb151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae81dae1",
   "metadata": {},
   "source": [
    "#### 8.\tExplain Chunking or shallow parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ce8f31",
   "metadata": {},
   "source": [
    "**- Chunking (aka. Shallow parsing) is to analyzing a sentence to identify the constituents (noun groups, verbs, verb groups, etc.).**\n",
    "\n",
    "<img src='https://www.bogotobogo.com/python/images/NLTK/Chunking/Token_Chunk.png' />\n",
    "\n",
    "**- Small boxes shows the word-level tokenization, and their parts of speech tagging.**\n",
    "\n",
    "**- Whereas the chunk is group of the tokens(which is sentences)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a01d641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b999e29b",
   "metadata": {},
   "source": [
    "#### 9.\tExplain Noun Phrase (NP) chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a25fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4433b045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56c0ebbc",
   "metadata": {},
   "source": [
    "#### 10.\tExplain Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2442ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
