{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47091910",
   "metadata": {},
   "source": [
    "# NLP Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb215e0f",
   "metadata": {},
   "source": [
    "###  1. Explain One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cded8310",
   "metadata": {},
   "source": [
    "**In Machine learning or in Deep learning, the model(algorithm) will take Quantitative Variables(continuous or discrete) valued variables as an input. But in real-world, not every problem will only have a Quantitative Variables, there also exists a Qualitative variables(categorical values) eg: \"cat\", So we need a way to convert the categorical values into numerical values or vectors, So one of the way to conversion is a One-Hot Encoding. It will create a columns equal to number of categories in the variable. eg: if a variable has a 'cat' and 'dog', it will create two columns, that will represent the values, it is similar to binary values representation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c1b6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "df = pd.DataFrame(data={\"Animals\": [\"cat\", \"dog\", \"cat\", \"cat\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6879ad32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Animals_cat</th>\n",
       "      <th>Animals_dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Animals_cat  Animals_dog\n",
       "0            1            0\n",
       "1            0            1\n",
       "2            1            0\n",
       "3            1            0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d29f23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7793c7fb",
   "metadata": {},
   "source": [
    "### 2. Explain Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40208e99",
   "metadata": {},
   "source": [
    "**The bag-of-words (BOW) model is a representation that turns arbitrary text into fixed-length vectors by counting how many times each word appears. This process is often referred to as vectorization..**\n",
    "\n",
    "eg: \n",
    "    \n",
    "    Review 1: Game of Thrones is an amazing tv series!\n",
    "\n",
    "    Review 2: Game of Thrones is the best tv series!\n",
    "\n",
    "    Review 3: Game of Thrones is so great\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*cHKkqYIhaYuYwuuhBiSlHw.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a794fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "400086df",
   "metadata": {},
   "source": [
    "### **3. Bag of N-Grams**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b80a93",
   "metadata": {},
   "source": [
    "**- A bag-of-n-grams model is a way to represent a document, similar to a [bag-of-words][/terms/bag-of-words/] model.**\n",
    "\n",
    "**- A bag-of-n-grams model represents a text document as an unordered collection of its n-grams.**\n",
    "\n",
    "**- A bag-of-n-grams model has the simplicity of the bag-of-words model, but allows the preservation of more word locality information.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eaa875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "244b8ef7",
   "metadata": {},
   "source": [
    "### **4. Explain TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1ca220",
   "metadata": {},
   "source": [
    "**- TF-IDF(Term Frequency-Inverse Document Frequency), is statistical method, mostly used in the Natural Language Processing. It measures how important a term is within a document relative to a collection of documents (i.e., relative to a corpus). Words within a text document are transformed into importance numbers by a text vectorization process. There are many different text vectorization scoring schemes, with TF-IDF being one of the most common.**\n",
    "\n",
    "**- It is a multiplication of two statistical methods, Term Freqeuncy, and Inverse Document Frequency.** \n",
    "\n",
    "**- Term Frequency: TF of a term or word is the number of times the term appears in a document compared to the total number of words in the document.**\n",
    "\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSA3IrMN-9cIqZWvdgki20rOpEFr92endHGkQ&usqp=CAU\" />\n",
    "\n",
    "**- Inverse Document Frequency: DF of a term reflects the proportion of documents in the corpus that contain the term. Words unique to a small percentage of documents (e.g., technical jargon terms) receive higher importance values than words common across all documents (e.g., a, the, and).**\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSrAE9DJvwRSDFrPUtUwsRM1WRpmvYDu2wYlQ&usqp=CAU\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b96365d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0b81a07",
   "metadata": {},
   "source": [
    "### **5. What is OOV problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d841477a",
   "metadata": {},
   "source": [
    "**- OOV (Out-Of_Vocabulary) error, is a problem arises when the word(term) that is not present in the training examples(documents), but it is present in the testing(inference) document. This problem mostly occur, bcoz of the training the model with a smaller dataset.**\n",
    "\n",
    "**- Word Embeddings encode the relationships between words through vector representations of the words. These word vectors are analogous to the meaning of the word. A limitation of word embeddings are that, they are learned by the Natural Language Model (word2vec, GloVe and the like) and therefore words must have been seen in the training data before, in order to have an embedding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c20c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58738a6f",
   "metadata": {},
   "source": [
    "### **6. What are word embeddings?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2411e579",
   "metadata": {},
   "source": [
    "**- Word Embedding, encode the relationship between the words in the corpus, throigh a vector representation of the words. These Word Vectors are analogous to the meaning of the word. As, we know the machine learning models only takes a Numerical Values, we need to convert the categorical data into numerical data, one of the way is using of word embedding. It also encode the semantic information of the words.**\n",
    "\n",
    "**- It will encode the words with the similar information, to be close to each other in the embedding space or vector space. Vector Representation of those words will be closer to each other.**\n",
    "\n",
    "**- Some of Word Embedding methods are, `Word2Vec, Glove, Bert, FastText, ELMo, etc.`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3738f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "047bdd2e",
   "metadata": {},
   "source": [
    "### **7. Explain Continuous bag of words (CBOW)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d641c04f",
   "metadata": {},
   "source": [
    "**- CBOW, is used to predict the target word, with fixed window of context words as an input.**\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:786/format:webp/1*cuOmGT7NevP9oJFJfVpRKA.png\" />\n",
    "\n",
    "**- CBOW, learns a better syntatic relationship, between the words. Eg: for a `word: dog`, it would retrieve a morphologically,  similar words like plurals, `i.e. 'dog'`**\n",
    "\n",
    "**- It will predict the target word, given the context of the surronding words. It is implemented using a feed forward neural network, where input is a set of surronding(context) words, and output is a target word.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c840f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code example for cbow\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "\n",
    "nltk.download('brown')\n",
    "data = brown.sents() \n",
    "\n",
    "model = Word2Vec()\n",
    "model.train(data, total_examples=len(data), epochs=20)\n",
    "word_vectors = model.wv\n",
    "\n",
    "similarity = word_vectors.similarity('woman', 'man')\n",
    "print(f\"Similarity between 'woman' and 'man': {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = model[model.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.scatter(result[:50, 0], result[:50, 1])\n",
    "words = list(model.wv.vocab)[:50]\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]), fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0082264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6a56096",
   "metadata": {},
   "source": [
    "### **8.What is Skip-grams**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734ee3f3",
   "metadata": {},
   "source": [
    "**- Skip-grams, trains a feed forward neural network, to predict the context words(surronding words), with the target as a input.**\n",
    "\n",
    "**- Both CBOW and Skip-grams, are used in word2vec, where the end goal of word2vec, is not to use the entire model as it is, but to use the weights of the model for each word in the vocabulary, which is a better representation of the words.**\n",
    "\n",
    "**- CBOW, will learns the better syntatic relationship, between the words. Whereas, the Skip-grams, learns the better semantic relationship.**\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:786/format:webp/1*cuOmGT7NevP9oJFJfVpRKA.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a7db8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d14ed34",
   "metadata": {},
   "source": [
    "### **9. Explain Glove**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5590036c",
   "metadata": {},
   "source": [
    "**“GloVe is a count-based, unsupervised learning model that uses co-occurrence (how frequently two words appear together) statistics at a Global level to model the vector representations of words.”**\n",
    "\n",
    "**-Since the statistics are captured at a global level directly by the model, it is named as ‘Global Vectors’ model.**\n",
    "\n",
    "**-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf9401d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
