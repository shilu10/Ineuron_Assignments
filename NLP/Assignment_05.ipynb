{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc44854",
   "metadata": {},
   "source": [
    "# Assignment 05 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35eaef1",
   "metadata": {},
   "source": [
    "#### 1.\tWhat are Sequence-to-sequence models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a94dd",
   "metadata": {},
   "source": [
    "**Sequence-to-Sequence models are also known as a Encoder-Decoder models, where the model takes sequence as a input and returns the sequence as a output. We have many application that needs a sequence-to-sequence models, like neural language translation, video captioning, etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d432c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c478edb2",
   "metadata": {},
   "source": [
    "#### 2.\tWhat are the Problem with Vanilla RNNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ae6d41",
   "metadata": {},
   "source": [
    "#### Problems:\n",
    "    \n",
    "    `1. Vanishing and exploding gradients.`\n",
    "    `2. RNN, cannot be stacked up.`\n",
    "    `3. slow and complex training procedure.`.\n",
    "    `4. Difficult to process longer sequence.`\n",
    "\n",
    "\n",
    "#### Vanishing and exploding Gradients:\n",
    "    Vanishing gradient, is common problem in neural network, where the gradient calulated from the objective function, will become smaller and smaller while during the backpropagation because of larger depth of the model. so the parameter of the network won't be updated during the backpropagation which will leads to the problem of model is not learning.(When Sequence is longer, vanishing problem arises in rnn network)\n",
    "    \n",
    "    Exploding Gradient, where the gradient value calculated from the objective function becomes very larger, so the updating the model parameter becomes unstable, after sometime the gradient values becomes NULL.\n",
    "    \n",
    "#### RNN, cannot be stacked up.`\n",
    "    The number one problem when it comes to parallelizing the trainings in RNN or a simple stack up of training is due to the fundamental characteristic of RNN, i.e., they are connected. What this means is RNNs require the output of the previous node to do the computation over the present node. Due to this connection, RNNs are not suitable for parallelizing or stacking up with other models. The overall computational expense that goes on can never be justified with any accuracy gain\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69491a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb759d62",
   "metadata": {},
   "source": [
    "#### 3.\tWhat is Gradient clipping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db22f60",
   "metadata": {},
   "source": [
    "    Gradient clipping is a technique to prevent exploding gradients in very deep networks, usually in recurrent neural networks. A neural network is a learning algorithm, also called neural network or neural net, that uses a network of functions to understand and translate data input into a specific output. This type of learning algorithm is designed based on the way neurons function in the human brain. There are many ways to compute gradient clipping, but a common one is to rescale gradients so that their norm is at most a particular value. With gradient clipping, pre-determined gradient threshold be introduced, and  then gradients norms that exceed this threshold are scaled down to match the norm.  This prevents any gradient to have norm greater than the threshold and thus the gradients are clipped.  There is an introduced bias in the resulting values from the gradient, but gradient clipping can keep things stable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4218d4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a639b7b3",
   "metadata": {},
   "source": [
    "#### 4.\tExplain Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e114e9fe",
   "metadata": {},
   "source": [
    "    Attention is one of the most researched concepts in the domain of deep learning for problems such as neural machine translation and image captioning. There are certain supporting concepts that help better explain the attention mechanism idea as a whole, such as Seq2Seq models, encoders, decoders, hidden states, context vectors, and so on.\n",
    "\n",
    "    When defining attention in simple terms, it refers to focusing on a certain component of the input problem and taking greater notice of it. The DL-based attention mechanism is also based on directing your focus, and paying greater attention to specific factors of a problem when processing data relevant to the problem.\n",
    "\n",
    "    Let us consider a sentence in English: \"I hope you are doing well\".\n",
    "\n",
    "    Our goal is to translate this sentence into Spanish. So, while the input sequence is the English sentence, the output sequence is supposed to be \"Espero que lo estás haciendo bien\".\n",
    "\n",
    "    For each word in the output sequence, the attention mechanism maps the relevant words in the input sequence. So, \"Espero\" in the output sequence will be mapped to \"I hope\" in the input sequence. \n",
    "\n",
    "    Higher 'weights' or relevance are assigned to input sequence words in relation to the appropriate words in the output sequence. The accuracy of output prediction is enhanced by doing this as the attention model is more capable of producing relevant output.\n",
    "    \n",
    "    types of attention:\n",
    "        1. Generalized attention.\n",
    "        2. Self attention.\n",
    "        3. mult-=head attention.\n",
    "        4. cross attention.\n",
    "        5. additive attention.\n",
    "        6. global attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d0589",
   "metadata": {},
   "source": [
    "#### 5.\tExplain Conditional random fields (CRFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ed65df",
   "metadata": {},
   "source": [
    "    CRF is a discriminant model for sequences data similar to MEMM. It models the dependency between each state and the entire input sequences. Unlike MEMM, CRF overcomes the label bias issue by using global normalizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2595683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34fa3100",
   "metadata": {},
   "source": [
    "#### 6.\tExplain self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e7df99",
   "metadata": {},
   "source": [
    "    We can think of self-attention as a mechanism that enhances the information content of an input embedding by including information about the input’s context. In other words, the self-attention mechanism enables the model to weigh the importance of different elements in an input sequence and dynamically adjust their influence on the output. This is especially important for language processing tasks, where the meaning of a word can change based on its context within a sentence or document.\n",
    "    \n",
    "\n",
    "<img src='https://jalammar.github.io/images/t/transformer_self-attention_visualization.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb6c6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "601b0899",
   "metadata": {},
   "source": [
    "#### 7.\tWhat is Bahdanau Attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a8b812",
   "metadata": {},
   "source": [
    "    Inspired by the idea of learning to align, Bahdanau et al. (2014) proposed a differentiable attention model without the unidirectional alignment limitation. When predicting a token, if not all the input tokens are relevant, the model aligns (or attends) only to parts of the input sequence that are deemed relevant to the current prediction. This is then used to update the current state before generating the next token. While quite innocuous in its description, this Bahdanau attention mechanism has arguably turned into one of the most influential ideas of the past decade in deep learning, giving rise to Transformers (Vaswani et al., 2017) and many related new architectures.\n",
    "\n",
    "#### Sequence-Sequence without attention\n",
    "<img src=\"https://d2l.ai/_images/seq2seq-state.svg\" />\n",
    "\n",
    "#### Sequence-Sequence with bahdanau attention\n",
    "<img src=\"https://d2l.ai/_images/seq2seq-details-attention.svg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ac39a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ba4026d",
   "metadata": {},
   "source": [
    "#### 8.\tWhat is a Language Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bca741",
   "metadata": {},
   "source": [
    "    - Models that assign probabilities to sequences of words are called language models(LMs).\n",
    "    - The simplest language model that assigns probabilities to sentences and sequences of words is the n-gram.\n",
    "    - An n-gram is a sequence of N words:\n",
    "        – A 1-gram (unigram) is a single word sequence of words like “please” or “ turn”.\n",
    "        – A 2-gram (bigram) is a two-word sequence of words like “please turn”, “turn your”, or\n",
    "        ”your homework”.\n",
    "        – A 3-gram (trigram) is a three-word sequence of words like “please turn your”, or “turn your\n",
    "        homework”.\n",
    "    - We can use n-gram models to estimate the probability of the last word of an n-gram given the previous words, and also to assign probabilities to entire word sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f56ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc53fbdb",
   "metadata": {},
   "source": [
    "#### 9.\tWhat is Multi-Head Attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fcd420",
   "metadata": {},
   "source": [
    "    The paper further refined the self-attention layer by adding a mechanism called “multi-headed” attention. This improves the performance of the attention layer in two ways:\n",
    "\n",
    "        1. It expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. If we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, it would be useful to know which word “it” refers to.\n",
    "\n",
    "        2. It gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.\n",
    "        \n",
    "        \n",
    " <img src='https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png' />\n",
    " <img src='https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd426c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0abe2b7a",
   "metadata": {},
   "source": [
    "#### 10. What is Bilingual Evaluation Understudy (BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61903f9f",
   "metadata": {},
   "source": [
    "    BLEU (BiLingual Evaluation Understudy) is a metric for automatically evaluating machine-translated text. The BLEU score is a number between zero and one that measures the similarity of the machine-translated text to a set of high quality reference translations. A value of 0 means that the machine-translated output has no overlap with the reference translation (low quality) while a value of 1 means there is perfect overlap with the reference translations (high quality).\n",
    "    \n",
    "    It has been shown that BLEU scores correlate well with human judgment of translation quality. Note that even human translators do not achieve a perfect score of 1.0.\n",
    "\n",
    "    We, can interpret BLEU, with the decimal values or with the whole number, whole numbers provides a better representation, and also the human translator cannot able to achieve a bleu score of 1.\n",
    "    \n",
    "    However, as a rough guideline, the following interpretation of BLEU scores (expressed as percentages rather than decimals) might be helpful.\n",
    "\n",
    "<img src='https://cloud.google.com/translate/automl/docs/images/bleu_score_range.png' />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d12c0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
